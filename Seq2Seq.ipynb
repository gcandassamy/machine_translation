{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SEED = 42\n",
    "random.seed(MANUAL_SEED)\n",
    "np.random.seed(MANUAL_SEED)\n",
    "torch.cuda.manual_seed(MANUAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\") # A subset of Multi30K dataset hosted on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = (dataset['train'], dataset['validation'], dataset['test'])\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download these models before loading (use > python -m spacy download model_name)\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input, en_nlp, de_nlp,  max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [t.text for t in en_nlp.tokenizer(input[\"en\"])][:max_length]\n",
    "    de_tokens = [t.text for t in de_nlp.tokenizer(input[\"de\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [t.lower() for t in en_tokens]\n",
    "        de_tokens = [t.lower() for t in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating tokens on the dataset\n",
    "max_length = 1000\n",
    "lower = True\n",
    "sos = \"<sos>\"\n",
    "eos = \"<eos>\"\n",
    "fn_kwargs = {\"de_nlp\": spacy_de, \"en_nlp\": spacy_en, \"max_length\": max_length, \"lower\":lower, \"sos_token\": sos, \"eos_token\":eos}\n",
    "\n",
    "train_data = train_data.map(tokenize, fn_kwargs=fn_kwargs)\n",
    "val_data = val_data.map(tokenize, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to have <unk> tokens in the training set in order to be able to handle unknown words in the test set\n",
    "- A nice trick to introduce '<unk>' tokens into our training set is to use the min_freq argument in the build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "unk = \"<unk>\"\n",
    "pad = \"<pad>\"\n",
    "spl_tokens = [unk, pad, sos, eos]\n",
    "en_voc = torchtext.vocab.build_vocab_from_iterator(train_data[\"en_tokens\"], min_freq=MIN_FREQ, specials=spl_tokens)\n",
    "de_voc = torchtext.vocab.build_vocab_from_iterator(train_data[\"de_tokens\"], min_freq=MIN_FREQ, specials=spl_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 tokens in the english vocabulary\n",
    "en_voc.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the index of a token use get_stoi\n",
    "en_voc.get_stoi()[\"the\"] # or simply use en_voc[\"the\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 5893\n",
      "German vocab size: 7853\n"
     ]
    }
   ],
   "source": [
    "# sizes of the vocabularies built...\n",
    "print(f'English vocab size: {len(en_voc)}')\n",
    "print(f'German vocab size: {len(de_voc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the locations for later use\n",
    "assert en_voc[unk] == de_voc[unk]\n",
    "assert en_voc[pad] == de_voc[pad]\n",
    "unk_idx = en_voc[unk]\n",
    "pad_idx = en_voc[pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling unknown tokens, we need to explicitly tell torchtext to map it to unk\n",
    "en_voc.set_default_index(unk_idx)\n",
    "de_voc.set_default_index(unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize_input(input, en_voc, de_voc):\n",
    "    en_idx = en_voc.lookup_indices(input[\"en_tokens\"])\n",
    "    de_idx = de_voc.lookup_indices(input[\"de_tokens\"])\n",
    "    return {\"en_idx\":en_idx, \"de_idx\": de_idx}\n",
    "\n",
    "fn_kwargs = {\"en_voc\": en_voc, \"de_voc\": de_voc}\n",
    "train_data = train_data.map(numerize_input, fn_kwargs=fn_kwargs)\n",
    "val_data = val_data.map(numerize_input, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numerize_input, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'en_idx': [2, 16, 24, 15, 25, 778, 17, 57, 80, 202, 1312, 5, 3],\n",
       " 'de_idx': [2, 18, 26, 253, 30, 84, 20, 88, 7, 15, 110, 7647, 3171, 4, 3]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_idx': tensor([   2,   16,   24,   15,   25,  778,   17,   57,   80,  202, 1312,    5,\n",
       "            3]),\n",
       " 'de_idx': tensor([   2,   18,   26,  253,   30,   84,   20,   88,    7,   15,  110, 7647,\n",
       "         3171,    4,    3]),\n",
       " 'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the token indices to Pytorch tensors...\n",
    "target_data_type = \"torch\"\n",
    "format_cols = [\"en_idx\", \"de_idx\"]\n",
    "\n",
    "train_data = train_data.with_format(type=target_data_type, columns=format_cols, output_all_columns=True)\n",
    "val_data = val_data.with_format(type=target_data_type, columns=format_cols, output_all_columns=True)\n",
    "test_data = test_data.with_format(type=target_data_type, columns=format_cols, output_all_columns=True)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][\"en_idx\"]) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining a batch of instances into a batch suitable for dataloader using a closure\n",
    "def call_create_batch(pad_idx):\n",
    "    def create_batch(batch):\n",
    "        batch_en_idx = [sentence[\"en_idx\"] for sentence in batch]\n",
    "        batch_de_idx = [sentence[\"de_idx\"] for sentence in batch]\n",
    "        batch_en_idx = nn.utils.rnn.pad_sequence(batch_en_idx, padding_value=pad_idx)\n",
    "        batch_de_idx = nn.utils.rnn.pad_sequence(batch_de_idx, padding_value=pad_idx)\n",
    "        batch = {\"en_idx\": batch_en_idx, \"de_idx\": batch_de_idx}\n",
    "        return batch\n",
    "    return create_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader...\n",
    "def create_dataloader(dataset, batch_size, pad_idx, shuffle=False):\n",
    "    collate_fn = call_create_batch(pad_idx)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataloader = create_dataloader(train_data, BATCH_SIZE, pad_idx, shuffle=True)\n",
    "val_dataloader = create_dataloader(val_data, BATCH_SIZE, pad_idx) # No need to shuffle for val / test splits\n",
    "test_dataloader = create_dataloader(test_data, BATCH_SIZE, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Encoder...\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, num_layers, dropout_prob):\n",
    "        '''\n",
    "        Args:\n",
    "        input_dim : input vocabulary size\n",
    "        embedding_dim : size of dense vectors produced by the embedding layer\n",
    "        hidden_size : dimensionality of hidden / cell states in the LSTM \n",
    "        num_layers : number of layers in the LSTM\n",
    "        dropout_prob: dropout probability to be used\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        # Caching these two parameters for verification later\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Defining the layers\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, dropout=dropout_prob)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input dim: (seq_length, N)\n",
    "        '''\n",
    "        embedded = self.dropout(self.embedding(input)) # (seq_length, N, embedding_dim)\n",
    "        _, (hidden_state, cell_state) = self.lstm(embedded)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "\n",
    "# Implementing the decoder...\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        # Defining the layers\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden_state, cell_state):\n",
    "        input = input.unsqueeze(0) # Prepend a 1 denoting seq length as we will be decoding one token at a time\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        outputs, (hidden_state, cell_state) = self.lstm(embedded, (hidden_state, cell_state))\n",
    "        preds = self.fc(outputs.squeeze(0))\n",
    "        return preds, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq with Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_with_TF(nn.Module):\n",
    "    def __init__(self, encoder, decoder, dev):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.dev = dev\n",
    "\n",
    "        # Following the architecture in the paper\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"Encoder & Decoder hidden dimensions don't match\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \"Number of layers inEncoder & Decoder don't match\"\n",
    "    \n",
    "    def forward(self, src, tgt, tf_ratio):\n",
    "        batch_sz = tgt.shape[1] # input --> (input_len, batch_size) and tgt --> (tgt_len, batch_size)\n",
    "        tgt_length = tgt.shape[0]\n",
    "        tgt_voc_sz = self.dec.output_size\n",
    "        outputs = torch.zeros(tgt_length, batch_sz, tgt_voc_sz).to(self.dev)\n",
    "        \n",
    "        hidden_state, cell_state = self.enc(src)\n",
    "        dec_input = tgt[0, :] # <sos> tokens\n",
    "\n",
    "        for t in range(1, tgt_length): # as we know the number of tokens in the target sentence\n",
    "            output, hidden, cell = self.dec(dec_input, hidden_state, cell_state)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < tf_ratio\n",
    "            # sampling from our prediction\n",
    "            top_preds = output.argmax(1)\n",
    "            dec_input = tgt[t] if teacher_force else top_preds\n",
    "        \n",
    "        return outputs # note that outputs[0] remains all zeros\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_voc)\n",
    "output_dim = len(en_voc)\n",
    "enc_embed_dim = 256\n",
    "dec_embed_dim = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2 # instead of 4 in the paper, just to save some time!\n",
    "drop_prob = 0.5\n",
    "\n",
    "encoder = Encoder(input_dim, enc_embed_dim, hidden_size, n_layers, drop_prob)\n",
    "decoder = Decoder(output_dim, dec_embed_dim, hidden_size, n_layers, drop_prob)\n",
    "model = Seq2Seq_with_TF(encoder, decoder, dev).to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization (following the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq_with_TF(\n",
       "  (enc): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (dec): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Seq2Seq model has 13,898,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Count the number of learnable parameters in our model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Our Seq2Seq model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, dataloader, optimizer, criterion, clip, tf_ratio, dev):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[\"de_idx\"].to(dev)\n",
    "        tgt = batch[\"en_idx\"].to(dev)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, tf_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader) # avg loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "- Note: We need to turn off teacher forcing during evaluation as we always wanna use our prediction as input at the subsequent time step in the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq2seq(model, dataloader, criterion, dev):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            src = batch[\"de_idx\"].to(dev)\n",
    "            tgt = batch[\"en_idx\"].to(dev)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:13<11:02, 73.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  5.4470 | Train perplexity: 232.0525 \n",
      "Validation loss:  5.2911 | Validation perplexity: 232.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:29<09:57, 74.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  5.0787 | Train perplexity: 160.5638 \n",
      "Validation loss:  5.4047 | Validation perplexity: 160.5638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:50<09:05, 77.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.9143 | Train perplexity: 136.2286 \n",
      "Validation loss:  5.2198 | Validation perplexity: 136.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:12<07:56, 79.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.8379 | Train perplexity: 126.2043 \n",
      "Validation loss:  5.2838 | Validation perplexity: 126.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:35<06:42, 80.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.7455 | Train perplexity: 115.0668 \n",
      "Validation loss:  5.2218 | Validation perplexity: 115.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:57<05:25, 81.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.6902 | Train perplexity: 108.8776 \n",
      "Validation loss:  5.1918 | Validation perplexity: 108.8776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [09:20<04:05, 81.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.6466 | Train perplexity: 104.2278 \n",
      "Validation loss:  5.1859 | Validation perplexity: 104.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [10:43<02:44, 82.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.6188 | Train perplexity: 101.3684 \n",
      "Validation loss:  5.1754 | Validation perplexity: 101.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [12:06<01:22, 82.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.5827 | Train perplexity: 97.7761 \n",
      "Validation loss:  5.2826 | Validation perplexity: 97.7761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [13:31<00:00, 81.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.4269 | Train perplexity: 83.6691 \n",
      "Validation loss:  5.1092 | Validation perplexity: 83.6691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "CLIP = 1.0\n",
    "TF_RATIO = 0.5\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(NUM_EPOCHS)):\n",
    "    train_loss = train_seq2seq(model, train_dataloader, optimizer, criterion, CLIP, TF_RATIO, dev)\n",
    "    val_loss = eval_seq2seq(model, val_dataloader, criterion, dev)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "    print(f\"Train loss: {train_loss: 7.4f} | Train perplexity: {np.exp(train_loss):7.4f} \")\n",
    "    print(f\"Validation loss: {val_loss: 7.4f} | Validation perplexity: {np.exp(train_loss):7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Loss: 5.118 | Test perplexity:  167.046 \n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss = eval_seq2seq(model, test_dataloader, criterion, dev)\n",
    "print(f\" Test Loss: {test_loss:.3f} | Test perplexity: {np.exp(test_loss): 7.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
